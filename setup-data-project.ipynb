{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f6d7cf4-f2dd-4004-b247-75907a5511d0",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "There are a lot of data projects available on the web (e.g., **[my list of data eng projects](https://www.startdataengineering.com/post/data-engineering-projects/)**). While these projects are great, starting from scratch to build your data project can be challenging. If you are \n",
    "\n",
    "> Wondering how to go from an idea to a production-ready data pipeline\n",
    "\n",
    "> Feeling overwhelmed by how all the parts of a data system fit together\n",
    "\n",
    "> Unsure that the pipelines you build are up to industry-standard\n",
    "\n",
    "If so, this post is for you! In it, we will go over how to build a data project step-by-step from scratch.\n",
    "\n",
    "By the end of this post, you will be able to quickly create data projects for any use case and see how the different parts of data systems work together. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2603c6cf-c3e5-4103-bf08-9b079a1d325a",
   "metadata": {},
   "source": [
    "# Parts of data project\n",
    "\n",
    "Most data engineering tool falls into one of the parts shown below (as explained in this [post](https://www.startdataengineering.com/post/parts-of-dataengineering/))\n",
    "\n",
    "![Data tools](./assets/images/data-tools.png)\n",
    "\n",
    "In this post, we will review the parts of a data project and select tools to build a data pipeline. While we chose TPCH data for this project, anyone can choose any data set they find interesting and follow the below steps to quickly build their data pipeline.\n",
    "\n",
    "### **Recommended reading**: **[What are the key parts of data engineering](https://www.startdataengineering.com/post/parts-of-dataengineering/)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ad6b23-067c-47d5-b8de-01b8a21e887c",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "\n",
    "The first step before you start should be defining precise requirements. Please work with the end users to define them (or define them yourself for side projects).\n",
    "\n",
    "We will go over a few key requirements below. \n",
    "\n",
    "### **Recommended reading**: **[this post that goes over how to gather requirements for data projects in detail!](https://www.startdataengineering.com/post/n-questions-data-pipeline-req/)**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1218fd-cae6-49e5-b1e4-2b8ac4eceb70",
   "metadata": {},
   "source": [
    "## Understand input datasets available\n",
    "\n",
    "Let's assume we are working with a car part seller database (tpch). The data is available in a duckdb database. See the data model below:\n",
    "\n",
    "![TPCH data model](./assets/images/tpch_erd.png)\n",
    "\n",
    "We can create fake input data using the [create_input_data.py](https://github.com/josephmachado/de_project/blob/main/setup/create_input_data.py) as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "196b6e1d-684a-4f1f-b048-7f9421e9ad55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up tpch and metadata db files\n",
      "Creating TPCH input data\n",
      "Creating metadata table\n"
     ]
    }
   ],
   "source": [
    "! python ./setup/create_input_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792878dc-081c-4d8c-9fcc-12e6b744891a",
   "metadata": {},
   "source": [
    "## Define what the output dataset will look like\n",
    "\n",
    "Let's assume that the `customer` team has asked us to create a dataset that they will use for outreach (think cold emails, calls, etc.). \n",
    "\n",
    "Upon discussion with the `customer` team, you discover that the output dataset requires the following columns:\n",
    "\n",
    "For each customer (i.e., one row per customer)\n",
    "\n",
    "1. **customer_key**: The unique identifier for the customer \n",
    "2. **customer_name**: The customer name\n",
    "3. **min_order_value**: The value of the order with the lowest value placed by this customer\n",
    "4. **max_order_value**: The value of the order with the highest value placed by this customer\n",
    "5. **avg_order_value**: The average value of all the orders placed by this customer  \n",
    "6. **avg_num_items_per_order**: The average number of items per order placed by this customer\n",
    "\n",
    "Let's write a simple query to see how we can get this (note that this process will take much longer with badly modeled input data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29118f12-3b06-41b3-952a-240fa7fa8408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌──────────────┬────────────────────┬─────────────────┬─────────────────┬────────────────────┬─────────────────────────┐\n",
       "│ customer_key │   customer_name    │ min_order_value │ max_order_value │  avg_order_value   │ avg_num_items_per_order │\n",
       "│    int64     │      varchar       │  decimal(15,2)  │  decimal(15,2)  │       double       │         double          │\n",
       "├──────────────┼────────────────────┼─────────────────┼─────────────────┼────────────────────┼─────────────────────────┤\n",
       "│          611 │ Customer#000000611 │        37857.53 │       226132.40 │  95871.07363636364 │       3.272727272727273 │\n",
       "│          818 │ Customer#000000818 │        23233.65 │       326565.37 │ 176473.52384615384 │       4.230769230769231 │\n",
       "│          163 │ Customer#000000163 │        49760.53 │       328804.37 │ 133322.93857142856 │       3.642857142857143 │\n",
       "│           34 │ Customer#000000034 │        50473.48 │       308920.79 │ 164393.27958333332 │                   4.625 │\n",
       "│         1078 │ Customer#000001078 │        17384.33 │       340782.49 │ 120262.49966666667 │      3.3666666666666667 │\n",
       "│          826 │ Customer#000000826 │        39398.24 │       331740.14 │       160536.14375 │       4.333333333333333 │\n",
       "│          167 │ Customer#000000167 │        17668.60 │       214721.32 │         145498.519 │                     3.9 │\n",
       "│          274 │ Customer#000000274 │        31756.21 │       308306.22 │ 173750.38333333333 │       4.555555555555555 │\n",
       "│          326 │ Customer#000000326 │        24067.70 │       254135.81 │ 136373.50222222222 │                     4.0 │\n",
       "│          331 │ Customer#000000331 │        29612.61 │       359455.08 │ 173025.32944444445 │       5.111111111111111 │\n",
       "│           ·  │         ·          │            ·    │           ·     │          ·         │               ·         │\n",
       "│           ·  │         ·          │            ·    │           ·     │          ·         │               ·         │\n",
       "│           ·  │         ·          │            ·    │           ·     │          ·         │               ·         │\n",
       "│          506 │ Customer#000000506 │         5002.26 │       188420.26 │ 114243.81444444445 │      3.3333333333333335 │\n",
       "│         1370 │ Customer#000001370 │         3182.79 │       211990.67 │       108629.78125 │                     3.0 │\n",
       "│         1388 │ Customer#000001388 │        51067.12 │       351086.78 │ 158131.73888888888 │       4.666666666666667 │\n",
       "│          454 │ Customer#000000454 │        30182.92 │       340337.98 │ 166113.28363636363 │       4.818181818181818 │\n",
       "│         1238 │ Customer#000001238 │       166921.49 │       304538.52 │ 225834.15666666668 │       5.833333333333333 │\n",
       "│         1154 │ Customer#000001154 │        58962.40 │       254602.02 │       144394.20125 │                   3.875 │\n",
       "│         1024 │ Customer#000001024 │        62230.79 │       298590.61 │ 157420.17933333333 │       5.133333333333334 │\n",
       "│         1352 │ Customer#000001352 │        76247.62 │       246330.37 │          178142.76 │                   5.125 │\n",
       "│          812 │ Customer#000000812 │        44819.53 │       258897.46 │ 151919.84833333333 │                     4.5 │\n",
       "│           32 │ Customer#000000032 │        13981.23 │       279545.76 │          152343.26 │       4.833333333333333 │\n",
       "├──────────────┴────────────────────┴─────────────────┴─────────────────┴────────────────────┴─────────────────────────┤\n",
       "│ 1000 rows (20 shown)                                                                                       6 columns │\n",
       "└──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple query to get the output dataset\n",
    "import duckdb\n",
    "con = duckdb.connect(\"tpch.db\")\n",
    "con.sql(\"\"\"\n",
    "WITH order_items AS (\n",
    "    SELECT\n",
    "        l_orderkey,\n",
    "        COUNT(*) AS item_count\n",
    "    FROM\n",
    "        lineitem\n",
    "    GROUP BY\n",
    "        l_orderkey\n",
    "),\n",
    "customer_orders AS (\n",
    "    SELECT\n",
    "        o.o_custkey,\n",
    "        o.o_orderkey,\n",
    "        o.o_totalprice,\n",
    "        oi.item_count\n",
    "    FROM\n",
    "        orders o\n",
    "    JOIN\n",
    "        order_items oi ON o.o_orderkey = oi.l_orderkey\n",
    ")\n",
    "SELECT\n",
    "    c.c_custkey AS customer_key,\n",
    "    c.c_name AS customer_name,\n",
    "    MIN(co.o_totalprice) AS min_order_value,\n",
    "    MAX(co.o_totalprice) AS max_order_value,\n",
    "    AVG(co.o_totalprice) AS avg_order_value,\n",
    "    AVG(co.item_count) AS avg_num_items_per_order\n",
    "FROM\n",
    "    customer c\n",
    "JOIN\n",
    "    customer_orders co ON c.c_custkey = co.o_custkey\n",
    "GROUP BY\n",
    "    c.c_custkey, c.c_name;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e66b874f-69b6-473c-be4c-ff0ab0e9bb6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionException",
     "evalue": "Connection Error: Connection already closed!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionException\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m con\u001b[38;5;241m.\u001b[39mclose() \n",
      "\u001b[0;31mConnectionException\u001b[0m: Connection Error: Connection already closed!"
     ]
    }
   ],
   "source": [
    "con.commit()\n",
    "con.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8063abf-6654-4f8b-b038-b3efdf05dcb8",
   "metadata": {},
   "source": [
    "## Define SLAs so stakeholders know what to expect\n",
    "\n",
    "SLAs stand for service level agreement. SLAs define what end-users can expect from your service(data pipeline, in our case). While there are multiple ways to define SLAs, the common ones for data systems are:\n",
    "\n",
    "1. `Data freshness`\n",
    "2. `Data accuracy`\n",
    "\n",
    "Let's assume that our stakeholders require the data to be no older than 12 hours. This means that your pipeline should run completely at least once every 12 hours. If we assume that the pipeline runs in 2 hours, we need to ensure that it is run at least every 10 hours so that the data is not older than 12 hours at any given time.\n",
    "\n",
    "For data accuracy, we should define what accurate data is. Let's define accuracy in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f25f7b-b3e6-4bda-ba4b-d1d98aae2b34",
   "metadata": {},
   "source": [
    "## Define checks to ensure the output dataset is usable\n",
    "\n",
    "We need to ensure that the data we produce is good enough for end-users to use. Typically, the data team works with end users to identify the critical metrics to check. \n",
    "\n",
    "Let's assume we have the following checks to ensure that the output dataset is accurate:\n",
    "\n",
    "1. **customer_key**: Has to be unique and not null\n",
    "2. **avg_***: columns should not differ by more than 5% compared to prior runs (across all customers)\n",
    "\n",
    "### **Recommended reading**: **[Types of data quality checks](https://www.startdataengineering.com/post/types-of-dq-checks/)** & **[Implementing data quality checks with Great Expectations](https://www.startdataengineering.com/post/implement_data_quality_with_great_expectations/)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d595d0-fd16-4766-b712-907648a36579",
   "metadata": {},
   "source": [
    "# Identify what tool to use to process data\n",
    "\n",
    "We have a plethora of tools to process data, including Apache Spark, Snowflake, Python, Polars, and DuckDB. We will use Polars to process our data because it is small. The Polars library is easy to install and use.\n",
    "\n",
    "### **Recommended reading**: **[Choosing tools for your data project](https://www.startdataengineering.com/post/choose-tools-dp/#41-requirement-x-component-framework)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e82ab1-b56f-4a4a-a100-bace58dfb9c4",
   "metadata": {},
   "source": [
    "# Data flow architecture\n",
    "\n",
    "Most data teams have their version of the 3-hop architecture. For example, dbt has its own version (stage, intermediate, mart), and Spark has medallion (bronze, silver, gold) architecture.\n",
    "\n",
    "You may be wondering why we need this data flow architecture when we have **[the results easily with a simple query shown here](./setup-data-project.ipynb#Columns-and-metics-needed-in-the-dataset-produced)**. \n",
    "\n",
    "While this is a simple example, in most real-world projects you want to have a standard, cleaned and modelled dataset(bronze) that can be use to create specialized dataset for end-users(gold). See below for how our data will flow:\n",
    "\n",
    "![Data Flow](./assets/images/dep-arch.png)\n",
    "\n",
    "### **Recommended reading**: **[Multi-hop architecture](https://www.startdataengineering.com/post/de_best_practices/#31-use-standard-patterns-that-progressively-transform-your-data)** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa70beaa-6dbc-412e-b468-96d72ca18622",
   "metadata": {},
   "source": [
    "## Bronze: Extract raw data and confine it to standard names and data types \n",
    "\n",
    "Since our dataset has data from customer, nation, region, order, and lineitem input datasets, we will bring those data into bronze tables. We will keep their names the same as the input datasets.\n",
    "\n",
    "Let's explore the input datasets and create our bronze datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08122bcc-e005-4139-8a36-b854ea30c16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read customer, order, and lineitem dataset from duckdb into Polars dataframe\n",
    "import duckdb\n",
    "import polars as pl\n",
    "\n",
    "con = duckdb.connect(\"tpch.db\")\n",
    "customer_df = con.sql(\"select * from customer\").pl()\n",
    "orders_df = con.sql(\"select * from orders\").pl()\n",
    "lineitem_df = con.sql(\"select * from lineitem\").pl()\n",
    "nation_df = con.sql(\"select * from nation\").pl()\n",
    "region_df = con.sql(\"select * from region\").pl()\n",
    "\n",
    "con.close() #close DuckDB connection\n",
    "\n",
    "# remove c_ and then rename custkey to customer_key\n",
    "cleaned_customer_df = customer_df.rename(lambda col_name: col_name[2:]).rename({\"custkey\": \"customer_key\"})\n",
    "\n",
    "# Remove the o_ and l_ from the order and lineitem table's column names\n",
    "# We also rename customer key and order key\n",
    "cleaned_orders_df = orders_df.rename(lambda col_name: col_name[2:]).rename({\"custkey\": \"customer_key\", \"orderkey\": \"order_key\"})\n",
    "cleaned_lineitem_df = lineitem_df.rename(lambda col_name: col_name[2:]).rename({\"orderkey\": \"order_key\"})\n",
    "\n",
    "# remove the n_ and r_ from the nation and region table's column names\n",
    "cleaned_nation_df = nation_df.rename(lambda col_name: col_name[2:])\n",
    "cleaned_region_df = region_df.rename(lambda col_name: col_name[2:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7c521a-e814-44da-9fc0-6b663ee4762e",
   "metadata": {},
   "source": [
    "## Silver: Model data for analytics\n",
    "\n",
    "In the silver layer, the datasets are modeled using one of the popular styles (e.g., Kimball, Data Vault, etc.). We will use Kimball's dimensional model, as it is the most commonly used one and can account for many use cases.\n",
    "\n",
    "### Data modeling\n",
    "\n",
    "We will create the following datasets\n",
    "\n",
    "1. **dim_customer**: A customer level table with all the necessary attributes of a customer. We will join nation and region data to the `cleaned_customer_df` to get all the attributes associated with a customer.\n",
    "2. **fct_orders**: An order level fact(an event that happened) table. This will be the same as `cleaned_orders_df` since the `orders` table has all the necessary details about the order and how it associates with dimension tables like `customer_key`.\n",
    "3. **fct_lineitem**: A lineitem (items that are part of an order) fact table. This table will be the same as `cleaned_lineitem_df` since the `lineitem` table has all the lineitem level details and keys to associate with dimension tables like `partkey` and `suppkey`.\n",
    "\n",
    "### **Recommended reading**: **[Data warehouse overview](https://www.startdataengineering.com/post/what-is-a-data-warehouse/#3-what-is-a-data-warehouse)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "640ea92d-31e3-4878-90aa-24eafb2d193d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create customer dimension by left-joining all the necessary data\n",
    "dim_customer = cleaned_customer_df\\\n",
    ".join(cleaned_nation_df, on=\"nationkey\", how=\"left\", suffix=\"_nation\")\\\n",
    ".join(cleaned_region_df, on=\"regionkey\", how=\"left\", suffix=\"_region\")\\\n",
    ".rename({\n",
    "    \"name_nation\": \"nation_name\",\n",
    "    \"name_region\": \"region_name\",\n",
    "    \"comment_nation\": \"nation_comment\",\n",
    "    \"comment_region\": \"region_comment\"\n",
    "})\n",
    "\n",
    "# Most fact tables are direct data from the app\n",
    "fct_orders = cleaned_orders_df\n",
    "fct_lineitem = cleaned_lineitem_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04941142-588b-443e-874c-ed9acf153276",
   "metadata": {},
   "source": [
    "## Gold: Create tables for end-users\n",
    "\n",
    "The gold layer contains datasets required for the end user. The user-required datasets are fact tables joined with dimension tables aggregated to the necessary grain. In real-world projects, multiple teams/users ask for datasets with differing grains from the same underlying fact and dimension tables. While you can join the necessary tables and aggregate them individually for each ask, it leads to repeated code and joins.\n",
    "\n",
    "To avoid this issue, companies typically do the following:\n",
    "\n",
    "1. **OBT**: This is a fact table with multiple dimension tables left joined with it.\n",
    "2. **pre-aggregated table**: The OBT table rolled up to the end user/team requested grain. The pre-aggregated dataset will be the dataset that the end user accesses. By providing the end user with the exact columns they need, we can ensure that all the metrics are in one place and issues due to incorrect metric calculations by end users are significantly reduced. These tables act as our end-users SOT (source of truth). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bb2a65-8fe5-4b14-969f-03cbc9317477",
   "metadata": {},
   "source": [
    "### OBT: Join the fact table with all its dimensions\n",
    "\n",
    "In our example, we have two fact tables, `fct_orders` and `fct_lineitem`. Since we only have one dimension, `dim_customer,` we can join `fct_orders` and `dim_customer` to create `wide_orders`. For our use case, we can keep `fct_lineitem` as `wide_lineitem`.\n",
    "\n",
    "That said, we can easily see a case where we might need to join `parts` and `supplier` data with `fct_lineitem` to get `wide_lineitem`. But since our use case doesn't require this, we can skip it!\n",
    "\n",
    "Let's create our OBT tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e7bbb84-9c84-48df-a653-09d13a38cbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create wide_orders table\n",
    "wide_orders = fct_orders.join(dim_customer, on=\"customer_key\", how=\"left\")\n",
    "\n",
    "# For our use case, we don't need more information at a lineitem level\n",
    "wide_lineitem = fct_lineitem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a11e605-96ba-48c2-a1f8-0a8227cff3f3",
   "metadata": {},
   "source": [
    "### Pre-aggregated tables: Aggregate OBTs to stakeholder-specific grain\n",
    "\n",
    "According to our **[data requirements](./setup-data-project.ipynb#Columns-and-metics-needed-in-the-dataset-produced)**, we need data from customer, orders, and lineitem. Since we already have customer and order data in `wide_orders`, we can join that with `wide_lineitem` to get the necessary data.\n",
    "\n",
    "We can call the final dataset `customer_outreach_metrics` (read **[this article that discusses the importance of naming](https://docs.getdbt.com/blog/on-the-importance-of-naming)**).\n",
    "\n",
    "Let's create our final dataset in Python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ea6cd9c-7556-4b67-af6d-068808222f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create customer_outreach_metrics\n",
    "\n",
    "# get the number of lineitems per order\n",
    "order_lineitem_metrics = wide_lineitem.group_by(pl.col(\"order_key\")).agg(pl.col(\"linenumber\").count().alias(\"num_lineitems\"))\n",
    "# join the above df with wide_orders and group by customer key in wide orders to get avg, min, max order value & avg num items per order\n",
    "customer_outreach_metrics = wide_orders\\\n",
    ".join(order_lineitem_metrics, on=\"order_key\", how=\"left\")\\\n",
    ".group_by(\n",
    "    pl.col(\"customer_key\"), \n",
    "    pl.col(\"name\").alias(\"customer_name\"))\\\n",
    ".agg(\n",
    "    pl.min(\"totalprice\").alias(\"min_order_value\"),\n",
    "    pl.max(\"totalprice\").alias(\"max_order_value\"),\n",
    "    pl.mean(\"totalprice\").alias(\"avg_order_value\"),\n",
    "    pl.mean(\"num_lineitems\").alias(\"avg_num_items_per_order\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a649f4c9-9070-4a13-85bd-507f2951c5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>customer_key</th><th>customer_name</th><th>min_order_value</th><th>max_order_value</th><th>avg_order_value</th><th>avg_num_items_per_order</th></tr><tr><td>i64</td><td>str</td><td>decimal[15,2]</td><td>decimal[15,2]</td><td>decimal[15,2]</td><td>f64</td></tr></thead><tbody><tr><td>958</td><td>&quot;Customer#000000958&quot;</td><td>31973.21</td><td>314926.50</td><td>null</td><td>4.388889</td></tr><tr><td>905</td><td>&quot;Customer#000000905&quot;</td><td>17681.24</td><td>284167.18</td><td>null</td><td>3.8</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 6)\n",
       "┌──────────────┬────────────────┬────────────────┬────────────────┬────────────────┬───────────────┐\n",
       "│ customer_key ┆ customer_name  ┆ min_order_valu ┆ max_order_valu ┆ avg_order_valu ┆ avg_num_items │\n",
       "│ ---          ┆ ---            ┆ e              ┆ e              ┆ e              ┆ _per_order    │\n",
       "│ i64          ┆ str            ┆ ---            ┆ ---            ┆ ---            ┆ ---           │\n",
       "│              ┆                ┆ decimal[15,2]  ┆ decimal[15,2]  ┆ decimal[15,2]  ┆ f64           │\n",
       "╞══════════════╪════════════════╪════════════════╪════════════════╪════════════════╪═══════════════╡\n",
       "│ 958          ┆ Customer#00000 ┆ 31973.21       ┆ 314926.50      ┆ null           ┆ 4.388889      │\n",
       "│              ┆ 0958           ┆                ┆                ┆                ┆               │\n",
       "│ 905          ┆ Customer#00000 ┆ 17681.24       ┆ 284167.18      ┆ null           ┆ 3.8           │\n",
       "│              ┆ 0905           ┆                ┆                ┆                ┆               │\n",
       "└──────────────┴────────────────┴────────────────┴────────────────┴────────────────┴───────────────┘"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_outreach_metrics.limit(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276b7e0c-d854-48f4-afc6-e57c7e57684c",
   "metadata": {},
   "source": [
    "# Data quality and code testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8edcd6-58b6-4606-9eeb-896aaa82f4d7",
   "metadata": {},
   "source": [
    "## Data quality implementation\n",
    "\n",
    "As part of our requirements, we saw that the output dataset needs to have \n",
    "1. Unique and distinct `customer_key`\n",
    "2. Variance in `avg_*` columns between runs should not be more than 5% (across all customers)\n",
    "\n",
    "While the first test is a simple check, the second one requires that we use the data from previous runs and compare it with the current run's data or store the sum(avg_*) of each run. Let's store the run-level metrics in a run_metadata table (in sqlite3).\n",
    "\n",
    "Our pipelines should run data quality checks before making the data available to your end users. This ensures that you can catch any issues before they can cause damage.\n",
    "\n",
    "![WAP pattern](./assets/images/wap.png)\n",
    "\n",
    "### **Recommended reading**: **[Types of data quality checks](https://www.startdataengineering.com/post/types-of-dq-checks/)**, **[Implementing data quality checks with Great Expectations](https://www.startdataengineering.com/post/implement_data_quality_with_great_expectations/)**, & **[Write-Audit-Publish pattern](https://www.startdataengineering.com/post/de_best_practices/#32-ensure-data-is-valid-before-exposing-it-to-its-consumers-aka-data-quality-checks)**\n",
    "\n",
    "Let's see how we can implement DQ checks in a Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0f878b9-8826-41a9-a271-50f2a540ed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# get current run's metrics\n",
    "curr_metrics = json.loads(\n",
    "    customer_outreach_metrics\\\n",
    "    .select(\n",
    "        pl.col(\"avg_num_items_per_order\").alias(\"sum_avg_num_items_per_order\"),\n",
    "        pl.col(\"avg_order_value\").alias(\"sum_avg_order_value\")\n",
    "    )\\\n",
    "    .sum()\\\n",
    "    .write_json())[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2095ed50-2afc-4860-bf8a-2e529ea8de0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store run metadata in a table\n",
    "import sqlite3\n",
    "\n",
    "# Connect to SQLite database\n",
    "conn = sqlite3.connect('metadata.db')\n",
    "\n",
    "# Create a cursor object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Insert data into the run_metadata table\n",
    "cursor.execute('''\n",
    "    INSERT INTO run_metadata (run_id, metadata)\n",
    "    VALUES (?, ?)\n",
    "''', ('2024-09-15-10-00', json.dumps(curr_metrics)))\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60034e3f-e014-4cf8-b938-2cf5d8564219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume that another run of the data pipeline has been completed\n",
    "\n",
    "# Get the most recent data from the run_metadata table\n",
    "import sqlite3\n",
    "\n",
    "# Connect to SQLite database\n",
    "conn = sqlite3.connect('metadata.db')\n",
    "\n",
    "# Create a cursor object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Fetch the most recent row based on run_id\n",
    "cursor.execute('''\n",
    "    SELECT * FROM run_metadata\n",
    "    ORDER BY run_id DESC\n",
    "    LIMIT 1\n",
    "''')\n",
    "\n",
    "# Get the result\n",
    "most_recent_row = cursor.fetchone()\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f6a003c-bec7-42b6-88fb-7a58aa6c9c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the most recent metric\n",
    "prev_metric = json.loads(most_recent_row[1])\n",
    "\n",
    "# get current metric\n",
    "# This assumes the pipeline is rerun\n",
    "curr_metric = json.loads(\n",
    "    customer_outreach_metrics\\\n",
    "    .select(\n",
    "        pl.col(\"avg_num_items_per_order\").alias(\"sum_avg_num_items_per_order\"),\n",
    "        pl.col(\"avg_order_value\").cast(int).alias(\"sum_avg_order_value\")\n",
    "    )\\\n",
    "    .sum()\\\n",
    "    .write_json())[0]\n",
    "\n",
    "# Compare with current data for variance percentage\n",
    "def percentage_difference(val1, val2):\n",
    "    if val1 == 0 and val2 == 0:\n",
    "        return 0.0\n",
    "    elif val1 == 0 or val2 == 0:\n",
    "        return 100.0\n",
    "    return abs((val1 - val2) / ((val1 + val2) / 2)) * 100\n",
    "\n",
    "prev_metric['sum_avg_order_value'] = int(float(prev_metric['sum_avg_order_value']))\n",
    "\n",
    "comparison = {}\n",
    "for key in curr_metric:\n",
    "    if key in prev_metric:\n",
    "        comparison[key] = percentage_difference(curr_metric[key], prev_metric[key])\n",
    "\n",
    "if prev_metric is None:\n",
    "    print('No prev metric')\n",
    "    \n",
    "# code to check if variance < 5\n",
    "for k, v in comparison.items():\n",
    "    if v >= 5:\n",
    "        raise Exception(f\"Difference for {k} is greater than 5%: {v}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b1d7790a-ee75-4c42-9e08-1bb90b6a0221",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "database is locked",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m comparison_json \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(comparison)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Insert data into the run_metadata table\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'''\u001b[39;49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;43m    INSERT INTO run_metadata (run_id, metadata)\u001b[39;49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;43m    VALUES (?, ?)\u001b[39;49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;43m'''\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_timestamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomparison_json\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Commit the changes and close the connection\u001b[39;00m\n\u001b[1;32m     23\u001b[0m conn\u001b[38;5;241m.\u001b[39mcommit()\n",
      "\u001b[0;31mOperationalError\u001b[0m: database is locked"
     ]
    }
   ],
   "source": [
    "# Insert current run data into the run_metadata table\n",
    "# Store run metadata in a table\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "\n",
    "# Get current timestamp and format it\n",
    "current_timestamp = datetime.now().strftime('%Y-%m-%d-%H-%M')\n",
    "\n",
    "# Connect to SQLite database\n",
    "conn = sqlite3.connect('metadata.db')\n",
    "\n",
    "# Create a cursor object\n",
    "cursor = conn.cursor()\n",
    "comparison_json = json.dumps(comparison)\n",
    "\n",
    "# Insert data into the run_metadata table\n",
    "cursor.execute('''\n",
    "    INSERT INTO run_metadata (run_id, metadata)\n",
    "    VALUES (?, ?)\n",
    "''', (current_timestamp, comparison_json))\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944af0b7-1485-4f14-9662-4dba6a2a4d42",
   "metadata": {},
   "source": [
    "# Code organization\n",
    "\n",
    "Deciding how to organize your code can be overwhelming. Typically, companies use one of the following options to organize code:\n",
    "\n",
    "1. Based on multi-hop architecture. E.g. **[see this dbt folder structure](https://github.com/dbt-labs/jaffle_shop_duckdb/tree/duckdb/models)**\n",
    "2. Based on existing company standards.\n",
    "\n",
    "### Folder structure\n",
    "\n",
    "We can use the following folder structure for our use case(and most real-life projects).\n",
    "\n",
    "![Folder structure](./assets/images/folder.png)\n",
    "\n",
    "Each file under the elt folder will have the code necessary to generate that dataset. The above folder structure enables anyone new to the project to quickly understand where the code to create a certain dataset will be.\n",
    "\n",
    "### Code modularity\n",
    "\n",
    "We have the code to create the necessary tables; now, we have to put them into functions that are easy to use and maintain. \n",
    "\n",
    "#### **Recommended reading**: **[How to write modular python code](https://www.startdataengineering.com/post/code-patterns/#1-functional-design)**\n",
    "\n",
    "We will define the function `create_dataset` for each table in the Python script for our use case. Having a common named function will enable\n",
    "\n",
    "1. Consistent naming. For example: `dim_customer.create_dataset`, `customer_outreach_metrics.create_dataset`\n",
    "2. Pull out code commonalities into a base class. Moving code into a common base class will be covered in a future post.\n",
    "\n",
    "Let's see what functions we would want to include in the `etl/gold/pre-aggregated/customer_outreach_metrics.py.`\n",
    "\n",
    "**Note** We have moved code that involves reading/writing to metadata into **[de_project/utils/metadata.py](https://github.com/josephmachado/de_project/blob/main/de_project/utils/metadata.py)**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "66f58b52-dfcd-417f-add7-fbece129206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py\n",
    "import json\n",
    "\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "\n",
    "def get_latest_run_metrics():\n",
    "    # Connect to SQLite database\n",
    "    conn = sqlite3.connect(\"metadata.db\")\n",
    "\n",
    "    # Create a cursor object\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Fetch the most recent row based on run_id\n",
    "    cursor.execute(\n",
    "        \"\"\"\n",
    "        SELECT * FROM run_metadata\n",
    "        ORDER BY run_id DESC\n",
    "        LIMIT 1\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    # Get the result\n",
    "    most_recent_row = cursor.fetchone()\n",
    "\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "    return (\n",
    "        json.loads(most_recent_row[1])\n",
    "        if most_recent_row and len(most_recent_row) > 0\n",
    "        else None\n",
    "    )\n",
    "\n",
    "\n",
    "def insert_run_metrics(curr_metrics):\n",
    "    # Connect to SQLite database\n",
    "    conn = sqlite3.connect(\"metadata.db\")\n",
    "\n",
    "    # Create a cursor object\n",
    "    cursor = conn.cursor()\n",
    "    curr_metrics_json = json.dumps(curr_metrics)\n",
    "\n",
    "    current_timestamp = datetime.now().strftime('%Y-%m-%d-%H-%M')\n",
    "    # Insert data into the run_metadata table\n",
    "    cursor.execute(\n",
    "        \"\"\"\n",
    "        INSERT INTO run_metadata (run_id, metadata)\n",
    "        VALUES (?, ?)\n",
    "    \"\"\",\n",
    "        (current_timestamp, curr_metrics_json),\n",
    "    )\n",
    "\n",
    "    # Commit the changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a106863d-24ed-42fe-bece-0755c005e2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import polars as pl\n",
    "\n",
    "def create_dataset(wide_lineitem, wide_orders):\n",
    "    order_lineitem_metrics = wide_lineitem.group_by(pl.col(\"order_key\")).agg(\n",
    "        pl.col(\"linenumber\").count().alias(\"num_lineitems\")\n",
    "    )\n",
    "    return (\n",
    "        wide_orders.join(order_lineitem_metrics, on=\"order_key\", how=\"left\")\n",
    "        .group_by(pl.col(\"customer_key\"), pl.col(\"name\").alias(\"customer_name\"))\n",
    "        .agg(\n",
    "            pl.min(\"totalprice\").alias(\"min_order_value\"),\n",
    "            pl.max(\"totalprice\").alias(\"max_order_value\"),\n",
    "            pl.mean(\"totalprice\").alias(\"avg_order_value\"),\n",
    "            pl.mean(\"num_lineitems\").alias(\"avg_num_items_per_order\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def percentage_difference(val1, val2):\n",
    "    if val1 == 0 and val2 == 0:\n",
    "        return 0.0\n",
    "    elif val1 == 0 or val2 == 0:\n",
    "        return 100.0\n",
    "    return abs((val1 - val2) / ((val1 + val2) / 2)) * 100\n",
    "\n",
    "\n",
    "def check_no_duplicates(customer_outreach_metrics_df):\n",
    "    # check uniqueness\n",
    "    if (\n",
    "        customer_outreach_metrics_df.filter(\n",
    "            customer_outreach_metrics_df.select(pl.col(\"customer_key\")).is_duplicated()\n",
    "        ).shape[0]\n",
    "        > 0\n",
    "    ):\n",
    "        raise Exception(\"Duplicate customer_keys\")\n",
    "\n",
    "\n",
    "def check_variance(customer_outreach_metrics_df, perc_threshold=5):\n",
    "    prev_metric = get_latest_run_metrics()\n",
    "    if prev_metric is None:\n",
    "        return\n",
    "    prev_metric['sum_avg_order_value'] = int(float(prev_metric['sum_avg_order_value']))\n",
    "    curr_metric = json.loads(\n",
    "        customer_outreach_metrics_df.select(\n",
    "            pl.col(\"avg_num_items_per_order\").alias(\"sum_avg_num_items_per_order\"),\n",
    "            pl.col(\"avg_order_value\").cast(int).alias(\"sum_avg_order_value\"),\n",
    "        )\n",
    "        .sum()\n",
    "        .write_json()\n",
    "    )[0]\n",
    "    comparison = {}\n",
    "    for key in curr_metric:\n",
    "        if key in prev_metric:\n",
    "            comparison[key] = percentage_difference(curr_metric[key], prev_metric[key])\n",
    "\n",
    "    for k, v in comparison.items():\n",
    "        if v >= perc_threshold:\n",
    "            raise Exception(f\"Difference for {k} is greater than 5%: {v}%\")\n",
    "\n",
    "\n",
    "def validate_dataset(customer_outreach_metrics_df):\n",
    "    # data quality checks\n",
    "    check_no_duplicates(customer_outreach_metrics_df)\n",
    "    check_variance(customer_outreach_metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd37db0-7496-4dc0-b03e-7994af55564d",
   "metadata": {},
   "source": [
    "## Code testing\n",
    "\n",
    "We will use `pytest` to test our code. Let's write a test case to test the `create_dataset` function for the `dim_customer` dataset. The test ccode is at **[de_project/tests/unit/dim_customer.py](https://github.com/josephmachado/de_project/blob/main/de_project/tests/unit/test_dim_customer.py)**.\n",
    "\n",
    "### **Recommended reading**: **[How to use pytest to test your code](https://www.startdataengineering.com/post/code-patterns/#4-testing-with-pytest)**\n",
    "\n",
    "We can run the tests via the terminal using\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "52e33dae-f49b-4e4b-be52-d4e56319bb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.12.4, pytest-8.3.3, pluggy-1.5.0\n",
      "rootdir: /home/josephkevinmachado/code/de_project\n",
      "plugins: anyio-4.4.0\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "de_project/tests/unit/test_dim_customer.py \u001b[32m.\u001b[0m\u001b[32m                             [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.11s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python -m pytest de_project/tests/unit/test_dim_customer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2321849-0a2d-463d-9bf7-59a68dec8d0e",
   "metadata": {},
   "source": [
    "We will add the below code to our unit test case and run pytest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809a8f27-1f80-4665-94e6-4c67bb3eb9a5",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "In the next post, we will cover the following:\n",
    "\n",
    "1. Containerizing the pipeline\n",
    "2. Scheduling pipeline with Airflow\n",
    "3. Moving common parts of code to the base class\n",
    "4. Persisting tables in a cloud storage system (e.g. S3)\n",
    "5. Deploying the pipeline to the cloud with Terraform\n",
    "6. Setting up monitoring and alerting infrastructure\n",
    "7. Setting up a dashboard to visualize the output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0111003-0ab1-4c8c-ab22-c45a08a77d46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
